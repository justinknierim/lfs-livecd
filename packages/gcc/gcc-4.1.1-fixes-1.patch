Submitted By: Alexander E. Patrakov
Date: 2006-12-11
Initial Package Version: 4.1.1
Upstream Status: backport
Origin: GCC SVN
Description: Various upstream fixes

1) Remove this from build log with Make-3.81:
sed: -e expression #1, char 88: unterminated address regex
2) Fix crash of programs compiled with -Os -ffast-math
3) Fix some cases of miscompilation

Upstream bugzilla URLs:

http://gcc.gnu.org/bugzilla/show_bug.cgi?id=13685
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=27334
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=27616
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=27768
http://gcc.gnu.org/bugzilla/show_bug.cgi?id=28386

--- gcc-4.1.1/gcc/Makefile.in
+++ gcc-4.1.1/gcc/Makefile.in
@@ -3146,8 +3209,8 @@
 macro_list: s-macro_list; @true
 s-macro_list : $(GCC_PASSES) 
 	echo | $(GCC_FOR_TARGET) -E -dM - | \
-	  sed -n 's/^#define \([^_][a-zA-Z0-9_]*\).*/\1/p ; \
-		s/^#define \(_[^_A-Z][a-zA-Z0-9_]*\).*/\1/p' | \
+	  sed -n -e 's/^#define \([^_][a-zA-Z0-9_]*\).*/\1/p' \
+		 -e 's/^#define \(_[^_A-Z][a-zA-Z0-9_]*\).*/\1/p' | \
 	  sort -u > tmp-macro_list
 	$(SHELL) $(srcdir)/../move-if-change tmp-macro_list macro_list
 	$(STAMP) s-macro_list
--- gcc-4.1.1/gcc/config/i386/i386.c
+++ gcc-4.1.1/gcc/config/i386/i386.c
@@ -1502,12 +1502,10 @@
     }
 
   /* Validate -mpreferred-stack-boundary= value, or provide default.
-     The default of 128 bits is for Pentium III's SSE __m128, but we
-     don't want additional code to keep the stack aligned when
-     optimizing for code size.  */
-  ix86_preferred_stack_boundary = (optimize_size
-				   ? TARGET_64BIT ? 128 : 32
-				   : 128);
+     The default of 128 bits is for Pentium III's SSE __m128, We can't
+     change it because of optimize_size.  Otherwise, we can't mix object
+     files compiled with -Os and -On.  */
+  ix86_preferred_stack_boundary = 128;
   if (ix86_preferred_stack_boundary_string)
     {
       i = atoi (ix86_preferred_stack_boundary_string);
--- gcc-4.1.1/gcc/cse.c
+++ gcc-4.1.1/gcc/cse.c
@@ -528,6 +528,10 @@
 
 static struct table_elt *table[HASH_SIZE];
 
+/* Number of elements in the hash table.  */
+
+static unsigned int table_size;
+
 /* Chain of `struct table_elt's made so far for this function
    but currently removed from the table.  */
 
@@ -962,6 +966,8 @@
 	}
     }
 
+  table_size = 0;
+
 #ifdef HAVE_cc0
   prev_insn = 0;
   prev_insn_cc0 = 0;
@@ -1372,6 +1378,8 @@
   /* Now add it to the free element chain.  */
   elt->next_same_hash = free_element_chain;
   free_element_chain = elt;
+
+  table_size--;
 }
 
 /* Look up X in the hash table and return its table element,
@@ -1649,6 +1657,8 @@
 	}
     }
 
+  table_size++;
+
   return elt;
 }
 
@@ -3441,10 +3451,10 @@
   return x;
 }
 
-/* Fold MEM.  */
+/* Fold MEM.  Not to be called directly, see fold_rtx_mem instead.  */
 
 static rtx
-fold_rtx_mem (rtx x, rtx insn)
+fold_rtx_mem_1 (rtx x, rtx insn)
 {
   enum machine_mode mode = GET_MODE (x);
   rtx new;
@@ -3607,6 +3617,51 @@
   }
 }
 
+/* Fold MEM.  */
+
+static rtx
+fold_rtx_mem (rtx x, rtx insn)
+{
+  /* To avoid infinite oscillations between fold_rtx and fold_rtx_mem,
+     refuse to allow recursion of the latter past n levels.  This can
+     happen because fold_rtx_mem will try to fold the address of the
+     memory reference it is passed, i.e. conceptually throwing away
+     the MEM and reinjecting the bare address into fold_rtx.  As a
+     result, patterns like
+
+       set (reg1)
+	   (plus (reg)
+		 (mem (plus (reg2) (const_int))))
+
+       set (reg2)
+	   (plus (reg)
+		 (mem (plus (reg1) (const_int))))
+
+     will defeat any "first-order" short-circuit put in either
+     function to prevent these infinite oscillations.
+
+     The heuristics for determining n is as follows: since each time
+     it is invoked fold_rtx_mem throws away a MEM, and since MEMs
+     are generically not nested, we assume that each invocation of
+     fold_rtx_mem corresponds to a new "top-level" operand, i.e.
+     the source or the destination of a SET.  So fold_rtx_mem is
+     bound to stop or cycle before n recursions, n being the number
+     of expressions recorded in the hash table.  We also leave some
+     play to account for the initial steps.  */
+
+  static unsigned int depth;
+  rtx ret;
+
+  if (depth > 3 + table_size)
+    return x;
+
+  depth++;
+  ret = fold_rtx_mem_1 (x, insn);
+  depth--;
+
+  return ret;
+}
+
 /* If X is a nontrivial arithmetic operation on an argument
    for which a constant value can be determined, return
    the result of operating on that value, as a constant.
@@ -4220,21 +4275,23 @@
 	    {
 	      int is_shift
 		= (code == ASHIFT || code == ASHIFTRT || code == LSHIFTRT);
-	      rtx y = lookup_as_function (folded_arg0, code);
-	      rtx inner_const;
+	      rtx y, inner_const, new_const;
 	      enum rtx_code associate_code;
-	      rtx new_const;
 
-	      if (y == 0
-		  || 0 == (inner_const
-			   = equiv_constant (fold_rtx (XEXP (y, 1), 0)))
-		  || GET_CODE (inner_const) != CONST_INT
-		  /* If we have compiled a statement like
-		     "if (x == (x & mask1))", and now are looking at
-		     "x & mask2", we will have a case where the first operand
-		     of Y is the same as our first operand.  Unless we detect
-		     this case, an infinite loop will result.  */
-		  || XEXP (y, 0) == folded_arg0)
+	      y = lookup_as_function (folded_arg0, code);
+	      if (y == 0)
+		break;
+
+	      /* If we have compiled a statement like
+		 "if (x == (x & mask1))", and now are looking at
+		 "x & mask2", we will have a case where the first operand
+		 of Y is the same as our first operand.  Unless we detect
+		 this case, an infinite loop will result.  */
+	      if (XEXP (y, 0) == folded_arg0)
+		break;
+
+	      inner_const = equiv_constant (fold_rtx (XEXP (y, 1), 0));
+	      if (!inner_const || GET_CODE (inner_const) != CONST_INT)
 		break;
 
 	      /* Don't associate these operations if they are a PLUS with the
--- gcc-4.1.1/gcc/tree-ssa-alias.c
+++ gcc-4.1.1/gcc/tree-ssa-alias.c
@@ -766,15 +766,22 @@
       struct alias_map_d *p_map = ai->pointers[i];
       tree tag = var_ann (p_map->var)->type_mem_tag;
       var_ann_t tag_ann = var_ann (tag);
+      tree var;
 
       p_map->total_alias_vops = 0;
       p_map->may_aliases = BITMAP_ALLOC (&alias_obstack);
 
+      /* Add any pre-existing may_aliases to the bitmap used to represent
+	 TAG's alias set in case we need to group aliases.  */
+      if (tag_ann->may_aliases)
+	for (j = 0; j < VARRAY_ACTIVE_SIZE (tag_ann->may_aliases); ++j)
+	  bitmap_set_bit (p_map->may_aliases,
+			  DECL_UID (VARRAY_TREE (tag_ann->may_aliases, j)));
+
       for (j = 0; j < ai->num_addressable_vars; j++)
 	{
 	  struct alias_map_d *v_map;
 	  var_ann_t v_ann;
-	  tree var;
 	  bool tag_stored_p, var_stored_p;
 	  
 	  v_map = ai->addressable_vars[j];
--- gcc-4.1.1/gcc/loop.c
+++ gcc-4.1.1/gcc/loop.c
@@ -8824,14 +8824,13 @@
 }
 
 
-/* Return false iff it is provable that biv BL plus BIAS will not wrap
-   at any point in its update sequence.  Note that at the rtl level we
-   may not have information about the signedness of BL; in that case,
-   check for both signed and unsigned overflow.  */
+/* Return false iff it is provable that biv BL will not wrap at any point
+   in its update sequence.  Note that at the RTL level we may not have
+   information about the signedness of BL; in that case, check for both
+   signed and unsigned overflow.  */
 
 static bool
-biased_biv_may_wrap_p (const struct loop *loop, struct iv_class *bl,
-		       unsigned HOST_WIDE_INT bias)
+biv_may_wrap_p (const struct loop *loop, struct iv_class *bl)
 {
   HOST_WIDE_INT incr;
   bool check_signed, check_unsigned;
@@ -8867,12 +8866,12 @@
   mode = GET_MODE (bl->biv->src_reg);
 
   if (check_unsigned
-      && !biased_biv_fits_mode_p (loop, bl, incr, mode, bias))
+      && !biased_biv_fits_mode_p (loop, bl, incr, mode, 0))
     return true;
 
   if (check_signed)
     {
-      bias += (GET_MODE_MASK (mode) >> 1) + 1;
+      unsigned HOST_WIDE_INT bias = (GET_MODE_MASK (mode) >> 1) + 1;
       if (!biased_biv_fits_mode_p (loop, bl, incr, mode, bias))
 	return true;
     }
@@ -10306,8 +10305,7 @@
 	 valid programs.  */
       /* Without lifetime analysis, we don't know how COMPARE will be
 	 used, so we must assume the worst.  */
-      if (code != EQ && code != NE
-	  && biased_biv_may_wrap_p (loop, bl, INTVAL (arg)))
+      if (code != EQ && code != NE && biv_may_wrap_p (loop, bl))
 	return 0;
 
       /* Try to replace with any giv that has constant positive mult_val
